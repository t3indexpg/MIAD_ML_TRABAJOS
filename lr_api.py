# -*- coding: utf-8 -*-
"""LR API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TvriatVKr_Zkck6vGeCllC-X1epKW9Uk
"""

import warnings
warnings.filterwarnings('ignore')

!pip install xgboost

# Importación librerías
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Carga de datos de archivo .csv
dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTrain_Spotify.csv')
dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTest_Spotify.csv', index_col=0)

# Importación de librerías necesarias
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- Análisis para dataTraining ---

# 1. Información general
print("Información general de dataTraining:")
dataTraining.info()

# 2. Estadísticas descriptivas
print("\nEstadísticas descriptivas de dataTraining:")
display(dataTraining.describe(include='all'))

# 3. Conteo de valores nulos
print("\nValores nulos por columna en dataTraining:")
print(dataTraining.isnull().sum())

# 4. Visualización de valores nulos
plt.figure(figsize=(12,6))
sns.heatmap(dataTraining.isnull(), cbar=False, cmap='mako')
plt.title('Mapa de valores nulos en dataTraining')
plt.show()

# --- Análisis para dataTesting ---

# 1. Información general
print("\nInformación general de dataTesting:")
dataTesting.info()

# 2. Estadísticas descriptivas
print("\nEstadísticas descriptivas de dataTesting:")
display(dataTesting.describe(include='all'))

# 3. Conteo de valores nulos
print("\nValores nulos por columna en dataTesting:")
print(dataTesting.isnull().sum())

# 4. Visualización de valores nulos
plt.figure(figsize=(12,6))
sns.heatmap(dataTesting.isnull(), cbar=False, cmap='mako')
plt.title('Mapa de valores nulos en dataTesting')
plt.show()

# Importación de librerías necesarias
import matplotlib.pyplot as plt
import seaborn as sns

# Lista de columnas numéricas clave para revisar
cols_outliers_training = ['duration_ms', 'tempo', 'loudness', 'popularity', 'key', 'mode', 'time_signature']
cols_outliers_testing = ['duration_ms', 'tempo', 'loudness', 'key', 'mode', 'time_signature']  # Sin 'popularity'

# --- Visualización de outliers en dataTraining ---
print("Boxplots de dataTraining:")

plt.figure(figsize=(15, 10))
for i, col in enumerate(cols_outliers_training, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=dataTraining[col])
    plt.title(f'{col}')
    plt.tight_layout()
plt.show()

# --- Visualización de outliers en dataTesting ---
print("Boxplots de dataTesting:")

plt.figure(figsize=(15, 8))  # Un poco más pequeño porque son menos variables
for i, col in enumerate(cols_outliers_testing, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=dataTesting[col])
    plt.title(f'{col}')
    plt.tight_layout()
plt.show()

# Importación de librerías necesarias
import matplotlib.pyplot as plt
import seaborn as sns

# Lista de variables con valores entre 0 y 1
vars_01 = [
    'danceability', 'energy', 'speechiness',
    'acousticness', 'instrumentalness',
    'liveness', 'valence'
]

# --- Visualización de distribución en dataTraining ---
print("Distribución de variables [0.0 - 1.0] en dataTraining:")

plt.figure(figsize=(14, 10))
for i, col in enumerate(vars_01, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=dataTraining[col])
    plt.title(col)
    plt.tight_layout()
plt.suptitle('Distribución de variables [0.0 - 1.0] en dataTraining', fontsize=16, y=1.02)
plt.show()

# --- Visualización de distribución en dataTesting ---
print("Distribución de variables [0.0 - 1.0] en dataTesting:")

plt.figure(figsize=(14, 10))
for i, col in enumerate(vars_01, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=dataTesting[col])
    plt.title(col)
    plt.tight_layout()
plt.suptitle('Distribución de variables [0.0 - 1.0] en dataTesting', fontsize=16, y=1.02)
plt.show()

# Importación de librerías necesarias
import matplotlib.pyplot as plt
import seaborn as sns

# Variables a graficar
variables_training = [
    'duration_ms', 'tempo', 'loudness', 'popularity', 'key', 'mode', 'time_signature',
    'danceability', 'energy', 'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence'
]

variables_testing = [
    'duration_ms', 'tempo', 'loudness', 'key', 'mode', 'time_signature',
    'danceability', 'energy', 'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence'
]  # Sin 'popularity'

# --- Histograma para dataTraining ---
print("Distribución de variables en dataTraining:")

num_cols = 3
num_rows = (len(variables_training) + num_cols - 1) // num_cols

plt.figure(figsize=(18, 18))
for idx, var in enumerate(variables_training):
    plt.subplot(num_rows, num_cols, idx + 1)
    sns.histplot(dataTraining[var], bins=30, kde=True, color='skyblue')
    plt.title(f'Distribución de {var}')
    plt.xlabel(var)
    plt.ylabel('Frecuencia')
    plt.grid(True)

plt.tight_layout()
plt.show()

# --- Histograma para dataTesting ---
print("Distribución de variables en dataTesting:")

num_cols = 3
num_rows = (len(variables_testing) + num_cols - 1) // num_cols

plt.figure(figsize=(18, 18))
for idx, var in enumerate(variables_testing):
    plt.subplot(num_rows, num_cols, idx + 1)
    sns.histplot(dataTesting[var], bins=30, kde=True, color='lightcoral')
    plt.title(f'Distribución de {var}')
    plt.xlabel(var)
    plt.ylabel('Frecuencia')
    plt.grid(True)

plt.tight_layout()
plt.show()

# Crear copias para trabajar
dataTraining_clean = dataTraining.copy()
dataTesting_clean = dataTesting.copy()

# --- Definir flags para dataTraining_clean ---

# Nuevo flag para time_signature
dataTraining_clean['time_signature_outlier'] = dataTraining_clean['time_signature'].apply(
    lambda x: 1 if (x > 3.5 and x <= 4.0) else 0
)

# Nuevo flag para tempo
dataTraining_clean['tempo_outlier'] = dataTraining_clean['tempo'].apply(
    lambda x: 1 if (x > 63.601 and x <= 159.004) else 0
)

# Nuevo flag para liveness
dataTraining_clean['liveness_outlier'] = dataTraining_clean['liveness'].apply(
    lambda x: 1 if (-0.001 < x <= 0.231) else 0
)

# Nuevo flag para instrumentalness
dataTraining_clean['instrumentalness_outlier'] = dataTraining_clean['instrumentalness'].apply(
    lambda x: 1 if (-0.001 < x <= 0.1) else 0
)

# Nuevo flag para duration_ms
dataTraining_clean['duration_ms_outlier'] = dataTraining_clean['duration_ms'].apply(
    lambda x: 1 if (13385.999 < x <= 535776.9) else 0
)

# --- Definir flags para dataTesting_clean ---

# Nuevo flag para time_signature
dataTesting_clean['time_signature_outlier'] = dataTesting_clean['time_signature'].apply(
    lambda x: 1 if (x > 3.5 and x <= 4.0) else 0
)

# Nuevo flag para tempo
dataTesting_clean['tempo_outlier'] = dataTesting_clean['tempo'].apply(
    lambda x: 1 if (x > 63.601 and x <= 159.004) else 0
)

# Nuevo flag para liveness
dataTesting_clean['liveness_outlier'] = dataTesting_clean['liveness'].apply(
    lambda x: 1 if (-0.001 < x <= 0.231) else 0
)

# Nuevo flag para instrumentalness
dataTesting_clean['instrumentalness_outlier'] = dataTesting_clean['instrumentalness'].apply(
    lambda x: 1 if (-0.001 < x <= 0.1) else 0
)

# Nuevo flag para duration_ms
dataTesting_clean['duration_ms_outlier'] = dataTesting_clean['duration_ms'].apply(
    lambda x: 1 if (13385.999 < x <= 535776.9) else 0
)

"""# ARTISTAS"""

import seaborn as sns
import matplotlib.pyplot as plt

# --- Correlación en dataTraining_clean ---
print("🔵 Matriz de correlación para dataTraining_clean:")

# Seleccionar solo variables numéricas
numeric_features_train = dataTraining_clean.select_dtypes(include=['int64', 'float64'])

# Calcular matriz de correlación
corr_matrix_train = numeric_features_train.corr()

# Visualizar
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix_train, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Matriz de correlación de variables numéricas - dataTraining_clean')
plt.tight_layout()
plt.show()

# --- Correlación en dataTesting_clean ---
print("\n🟠 Matriz de correlación para dataTesting_clean:")

# Seleccionar solo variables numéricas
numeric_features_test = dataTesting_clean.select_dtypes(include=['int64', 'float64'])

# Calcular matriz de correlación
corr_matrix_test = numeric_features_test.corr()

# Visualizar
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix_test, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Matriz de correlación de variables numéricas - dataTesting_clean')
plt.tight_layout()
plt.show()

# Importación de librerías
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression

selected_features = [
    'acousticness',
    'danceability',
    'energy',
    'loudness',
    'instrumentalness_outlier',

]

X = dataTraining_clean[selected_features]
y = dataTraining_clean['popularity']

# División en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Función para evaluar el modelo
def evaluar_modelo(nombre_modelo, modelo, X_test, y_test):
    y_pred = modelo.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"\nEvaluación del modelo: {nombre_modelo}")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

# 1. Modelo Linear Regressor
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
evaluar_modelo("Linear Regresssion", lr_model, X_test, y_test)


y_pred = lr_model.predict(X_test)
y_pred